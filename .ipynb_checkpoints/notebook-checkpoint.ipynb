{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ebeed5-6fab-4308-84e1-02b3399f5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dependency(dependency: str) -> bool:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pipenv\", \"install\", dependency])\n",
    "        return True\n",
    "    except e:\n",
    "        return False            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a25cf-fc74-43c4-9a71-37d1c592c3fb",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention\n",
    "\n",
    "As we read the paper, the authors focus on the scaled dot product attention first, this combined with multihead attention--which we'll come to next--forms the basis of this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d7aa964-c3e1-4c6d-9258-536e89081091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    queries: torch.Tensor, \n",
    "    keys: torch.Tensor, \n",
    "    values: torch.Tensor \n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        queries (torch.Tensor): Query matrix of shape (d_context, d_k).\n",
    "        keys (torch.Tensor): Key matrix of shape (d_context, d_k).\n",
    "        values (torch.Tensor): Value matrix of shape (d_context, d_k).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention-weighted sum of values.\n",
    "    \"\"\"\n",
    "    assert queries.shape[1] == keys.shape[1], \"Queries and keys must have the same number of dimensions\"\n",
    "    \n",
    "    # Compute attention scores\n",
    "    compatibility = queries @ keys.T\n",
    "    \n",
    "    # Scale by sqrt(d_k)\n",
    "    d_k = queries.shape[1]\n",
    "    stabilized_compat = compatibility / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    scaled_compat = F.softmax(stabilized_compat, dim=-1)\n",
    "    \n",
    "    # Compute attention output\n",
    "    return scaled_compat @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129504bc-38a3-4525-bc1f-a85d313c2d9e",
   "metadata": {},
   "source": [
    "And we'll run a quick test to make sure everything works :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51aef031-a371-435d-a3cf-022b0cf3c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0043, -0.0110, -0.0394,  ..., -0.0508,  0.0281, -0.0398],\n",
      "        [ 0.0653,  0.0198,  0.1255,  ..., -0.0096,  0.0336, -0.0329],\n",
      "        [-0.0497, -0.1065,  0.0174,  ..., -0.0755,  0.0563,  0.0345],\n",
      "        ...,\n",
      "        [ 0.1907, -0.0506,  0.1149,  ...,  0.0087, -0.0295, -0.0331],\n",
      "        [-0.0345, -0.0536, -0.0193,  ..., -0.0849,  0.0768,  0.0350],\n",
      "        [ 0.0262, -0.1209,  0.0337,  ..., -0.1057,  0.0646, -0.0657]],\n",
      "       dtype=torch.float64, grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "queries = torch.randn((d_model, d_k), dtype=torch.float64, requires_grad=True)\n",
    "keys = torch.randn((d_model, d_k), dtype=torch.float64, requires_grad=True)\n",
    "values = torch.randn((d_model, d_v), dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "attention = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "assert attention.shape[0] == d_model and attention.shape[1] == d_v, \"Attention has incorrect shape, should be: (n_token, d_value)\"\n",
    "\n",
    "print (attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee45739-2f26-4273-8bb6-15e1933994c7",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "With scaled dot product implemented, we can go ahead and implement multihead attention.\n",
    "\n",
    "We'll focus on this part first, since there is a lot to unpack here:  \n",
    "\"_Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries,\n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
    "linear projections to d_k, d_k and d_v dimensions, respectively_\"\n",
    "\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8714b47f-d5e4-43b9-8637-0f01d477cf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(20).view(2, 2, 5).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be974186-ce02-4e18-bc45-ee65158b2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Computes multi head attention\n",
    "        \"\"\"\n",
    "        d_k = d_v = x.shape[-1] / self.n_heads\n",
    "        query_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_k))\n",
    "        key_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_k))\n",
    "        value_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_v))\n",
    "        output_projection = nn.Parameter(torch.randn(d_v * self.n_heads, d_model))\n",
    "    \n",
    "        head_outputs = []\n",
    "        for i in range(num_heads):\n",
    "            q_proj = x @ query_projections[i]\n",
    "            k_proj = x @ key_projections[i]\n",
    "            v_proj = x @ value_projections[i]\n",
    "            head_output = scaled_dot_product_attention(q_proj, k_proj, v_proj)\n",
    "            head_outputs.append(head_output)\n",
    "    \n",
    "        concat_output = torch.concat(head_outputs, dim=-1)  \n",
    "        return concat_output @ output_projection    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0d45b7c-c319-49c8-84b8-b874d56e4ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7603,  0.6376,  0.1728,  ..., -0.8567,  0.2092, -1.5860],\n",
       "        [-1.3113, -0.1871, -0.4620,  ..., -0.1369,  0.7352, -1.1841],\n",
       "        [-0.3600,  1.3046, -1.3649,  ...,  0.4419,  0.4811, -0.4008],\n",
       "        ...,\n",
       "        [ 0.5887,  1.1117,  0.9526,  ..., -0.2709,  0.8288, -0.4260],\n",
       "        [ 0.8801,  0.9559, -0.5841,  ...,  0.7414,  0.8736,  1.6141],\n",
       "        [-0.2932, -0.7811,  1.6160,  ...,  0.6907, -0.9375, -0.9236]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_context = 64\n",
    "\n",
    "values = torch.randn(d_context, d_model)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dc035999-cc6c-4621-8f62-4512d25cd45e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got float\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m attention = MultiHeadAttention()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/attn_all_you_need-uKKxAT52/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/attn_all_you_need-uKKxAT52/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" \u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03mComputes multi head attention\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     12\u001b[39m d_k = d_v = x.shape[-\u001b[32m1\u001b[39m] / \u001b[38;5;28mself\u001b[39m.n_heads\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m query_projections = nn.Parameter(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_k\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     14\u001b[39m key_projections = nn.Parameter(torch.randn(\u001b[38;5;28mself\u001b[39m.n_heads, d_model, d_k))\n\u001b[32m     15\u001b[39m value_projections = nn.Parameter(torch.randn(\u001b[38;5;28mself\u001b[39m.n_heads, d_model, d_v))\n",
      "\u001b[31mTypeError\u001b[39m: randn(): argument 'size' failed to unpack the object at pos 3 with error \"type must be tuple of ints,but got float\""
     ]
    }
   ],
   "source": [
    "attention = MultiHeadAttention()\n",
    "attention(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d541c-b315-4c06-a74f-ae0f7099ab19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
