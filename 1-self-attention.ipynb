{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6051c1ab-b15e-4e87-a7e7-0a3c17068aeb",
   "metadata": {},
   "source": [
    "# Self Attention In Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b227f-870e-4f5d-a3db-99f5fd6c3baf",
   "metadata": {},
   "source": [
    "Self attention is the mechanism by which transformers learn to embed tokens in the input sequence with richer information from other tokens in the input sequence.\n",
    "\n",
    "> In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
    "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
    "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
    "translation quality after being trained for as little as twelve hours on eight P100 GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff45b4b-80fa-4522-bc65-0ef805f7e267",
   "metadata": {},
   "source": [
    "For now, we'll focus on the scaled dot product self attention mechanism. This mechanism is an introspective mechanism through which each token in a sequence is  imbued with information from other tokens within the same sequence with whom they have a learned relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467f8e6b-fed9-4058-aba6-1e7709d2afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fb87a-aebd-48fa-a7d3-261e50e8b093",
   "metadata": {},
   "source": [
    "Let's assume that the below tensor represents the embeddings of 64 possible subsequences, each 256 tokens in length, with each token having an embedding of 512 in length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7344b1-cd7a-4628-9137-ce7fa8571fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.2068e-01, -7.3312e-01, -3.1714e-01,  ...,  1.4042e+00,\n",
       "          -8.1633e-02,  2.3458e-02],\n",
       "         [ 1.1932e+00,  2.4985e+00, -7.4632e-01,  ...,  1.3621e+00,\n",
       "           2.1245e-01,  3.1476e-01],\n",
       "         [ 1.0477e-01,  8.8112e-01, -3.3800e-01,  ...,  8.6387e-02,\n",
       "          -1.3226e+00,  1.1879e+00],\n",
       "         ...,\n",
       "         [ 1.1468e+00, -5.8442e-01,  3.2023e-01,  ...,  1.3805e+00,\n",
       "           7.1827e-01, -2.0997e-01],\n",
       "         [-6.4437e-01,  9.8400e-01, -3.8320e-01,  ..., -5.1742e-01,\n",
       "          -1.2141e+00,  1.0175e+00],\n",
       "         [-1.3575e+00,  6.1631e-01, -1.1711e-01,  ...,  7.1650e-01,\n",
       "          -4.4457e-01, -1.2195e+00]],\n",
       "\n",
       "        [[-8.8185e-02,  7.6894e-01, -3.1657e-01,  ..., -4.5393e-01,\n",
       "          -5.9143e-01, -1.1855e+00],\n",
       "         [ 7.7802e-01, -2.2019e-02,  7.4432e-01,  ..., -6.5237e-01,\n",
       "           1.7421e+00,  2.1597e-01],\n",
       "         [ 2.0892e-02,  1.3980e+00, -4.8068e-01,  ..., -3.8617e-01,\n",
       "           1.1633e-02, -8.2565e-02],\n",
       "         ...,\n",
       "         [ 2.5897e-01, -5.2322e-01, -2.2494e-01,  ...,  7.9224e-01,\n",
       "           1.0793e-01,  8.2835e-01],\n",
       "         [-1.3081e+00,  5.5923e-01, -4.8574e-01,  ...,  1.5860e+00,\n",
       "           3.2752e-01, -3.8784e-01],\n",
       "         [-1.2414e-01, -6.8108e-01,  3.5976e-01,  ..., -6.0785e-01,\n",
       "           5.6313e-01, -7.8116e-01]],\n",
       "\n",
       "        [[-6.4286e-02,  4.7874e-01,  7.9267e-01,  ...,  1.7072e+00,\n",
       "          -6.6190e-01, -1.5404e+00],\n",
       "         [-2.5766e-01,  1.7498e-01,  8.5412e-01,  ..., -1.2372e+00,\n",
       "           1.9352e+00,  8.1395e-01],\n",
       "         [ 4.6413e-01,  5.4016e-01,  1.4844e+00,  ...,  2.6200e-01,\n",
       "          -1.0573e+00,  1.1998e+00],\n",
       "         ...,\n",
       "         [-7.1271e-01,  8.1084e-01, -2.4575e-01,  ...,  1.2581e+00,\n",
       "          -8.2708e-02,  1.4066e+00],\n",
       "         [-1.1534e+00,  8.8721e-02, -1.3986e+00,  ...,  1.8210e+00,\n",
       "           7.7532e-01, -5.4478e-01],\n",
       "         [-2.4912e-01, -7.0318e-01,  1.8346e+00,  ..., -6.2947e-01,\n",
       "           4.9618e-01,  1.2159e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.2174e+00, -2.2290e-01,  6.3387e-02,  ..., -2.1599e-02,\n",
       "          -4.6033e-01, -8.9870e-01],\n",
       "         [ 7.2814e-01,  3.1028e+00,  6.8730e-01,  ..., -6.7774e-01,\n",
       "          -4.1362e-01, -5.0187e-01],\n",
       "         [ 1.4353e+00, -1.2791e+00,  1.3511e+00,  ...,  1.4080e+00,\n",
       "           3.5672e-01, -4.0719e-01],\n",
       "         ...,\n",
       "         [ 8.6252e-01, -1.2586e+00, -4.5490e-01,  ...,  3.9718e-01,\n",
       "           8.6732e-01, -8.5532e-01],\n",
       "         [ 1.5242e+00,  7.9613e-01, -9.9092e-01,  ..., -1.4997e+00,\n",
       "          -4.8399e-01,  1.8777e+00],\n",
       "         [ 1.9394e+00, -1.6436e+00,  6.5103e-01,  ..., -9.7394e-02,\n",
       "          -6.4776e-01, -1.2816e+00]],\n",
       "\n",
       "        [[ 2.6246e-01, -4.5716e-02, -8.3101e-01,  ..., -1.0335e+00,\n",
       "          -6.8107e-01, -5.8436e-01],\n",
       "         [-1.1157e-01, -1.3621e-01, -1.9596e+00,  ...,  2.0095e+00,\n",
       "          -6.1100e-01, -2.4274e-02],\n",
       "         [-2.1479e+00, -9.2711e-01,  1.7638e+00,  ..., -8.1042e-01,\n",
       "           1.2412e+00, -2.9034e-02],\n",
       "         ...,\n",
       "         [-4.2020e-01,  2.1294e-01, -5.4305e-01,  ...,  2.7468e-01,\n",
       "           5.7481e-01,  5.5349e-01],\n",
       "         [-7.9359e-01, -1.0303e+00,  2.6168e-01,  ...,  1.7453e+00,\n",
       "           1.3311e-01, -3.3304e-02],\n",
       "         [-2.7853e-01,  4.4367e-01, -8.9210e-01,  ..., -1.6437e+00,\n",
       "           1.6021e+00,  1.2504e+00]],\n",
       "\n",
       "        [[-1.0544e+00, -1.7567e-01, -1.0568e+00,  ...,  4.3951e-01,\n",
       "          -1.5773e+00,  1.3995e+00],\n",
       "         [ 1.6949e+00, -6.5402e-01, -3.1078e-01,  ...,  2.4619e-01,\n",
       "           1.5242e+00,  2.6344e-01],\n",
       "         [-1.0042e+00,  1.0045e+00,  9.9659e-01,  ...,  3.5027e-01,\n",
       "          -3.0904e-01, -5.5028e-01],\n",
       "         ...,\n",
       "         [ 1.7078e-01,  2.0436e-01,  9.3914e-01,  ...,  3.1919e-01,\n",
       "          -2.8025e-01,  9.6185e-01],\n",
       "         [-1.5886e+00,  4.3613e-02, -8.9009e-01,  ...,  5.1107e-01,\n",
       "           1.2732e+00, -2.2636e-03],\n",
       "         [-5.1350e-01, -1.2637e+00, -3.3836e-01,  ...,  1.7052e-01,\n",
       "          -4.6383e-01,  2.5008e-01]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentence = torch.randn((64, 256, 512), dtype=torch.float)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9c2b1-ae59-465f-950f-6a8e4fe1287f",
   "metadata": {},
   "source": [
    "Self attention works by generating queries, keys and values from an input sequence and then applying those queries to the keys and values generated from the same sequence and adding the resulting values to the original sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def88a95-53c7-4b9e-9a3e-752c9d383e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, d_key: int, d_value: int) -> None:\n",
    "        super().__init__()\n",
    "        self.key_proj = nn.Parameter(torch.randn((d_model, d_key), dtype=torch.float))\n",
    "        self.value_proj = nn.Parameter(torch.randn((d_model, d_value), dtype=torch.float))\n",
    "        self.query_proj = nn.Parameter(torch.randn((d_model, d_key), dtype=torch.float))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
