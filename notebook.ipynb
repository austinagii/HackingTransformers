{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ebeed5-6fab-4308-84e1-02b3399f5a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_dependency(dependency: str) -> bool:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pipenv\", \"install\", dependency])\n",
    "        return True\n",
    "    except e:\n",
    "        return False            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a89fc2c2-c966-43a9-82e3-37fc11363f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a25cf-fc74-43c4-9a71-37d1c592c3fb",
   "metadata": {},
   "source": [
    "## Scaled Dot Product Attention\n",
    "\n",
    "As we read the paper, the authors focus on the scaled dot product attention first, this combined with multihead attention--which we'll come to next--forms the basis of this paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d7aa964-c3e1-4c6d-9258-536e89081091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    queries: torch.Tensor, \n",
    "    keys: torch.Tensor, \n",
    "    values: torch.Tensor \n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        queries (torch.Tensor): Query matrix of shape (d_context, d_k).\n",
    "        keys (torch.Tensor): Key matrix of shape (d_context, d_k).\n",
    "        values (torch.Tensor): Value matrix of shape (d_context, d_k).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention-weighted sum of values.\n",
    "    \"\"\"\n",
    "    assert queries.shape[1] == keys.shape[1], \"Queries and keys must have the same number of dimensions\"\n",
    "    \n",
    "    # Compute attention scores\n",
    "    compatibility = queries @ keys.T\n",
    "    \n",
    "    # Scale by sqrt(d_k)\n",
    "    d_k = queries.shape[1]\n",
    "    stabilized_compat = compatibility / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    scaled_compat = F.softmax(stabilized_compat, dim=-1)\n",
    "    \n",
    "    # Compute attention output\n",
    "    return scaled_compat @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129504bc-38a3-4525-bc1f-a85d313c2d9e",
   "metadata": {},
   "source": [
    "And we'll run a quick test to make sure everything works :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51aef031-a371-435d-a3cf-022b0cf3c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0006, -0.1030,  0.0279,  ..., -0.0876,  0.0201,  0.0039],\n",
      "        [ 0.1138, -0.0085, -0.0288,  ..., -0.0100, -0.0136, -0.0037],\n",
      "        [ 0.0294, -0.0451, -0.0455,  ...,  0.0154, -0.0138,  0.0649],\n",
      "        ...,\n",
      "        [ 0.0653, -0.1240, -0.0054,  ..., -0.0147,  0.0097,  0.0454],\n",
      "        [ 0.1139, -0.0221, -0.0298,  ..., -0.0383,  0.0234, -0.0043],\n",
      "        [ 0.0297,  0.0306, -0.0823,  ..., -0.0151,  0.0243,  0.1242]],\n",
      "       dtype=torch.float64, grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "d_k = 64\n",
    "d_v = 64\n",
    "\n",
    "queries = torch.randn((d_model, d_k), dtype=torch.float64, requires_grad=True)\n",
    "keys = torch.randn((d_model, d_k), dtype=torch.float64, requires_grad=True)\n",
    "values = torch.randn((d_model, d_v), dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "attention = scaled_dot_product_attention(queries, keys, values)\n",
    "\n",
    "assert attention.shape[0] == d_model and attention.shape[1] == d_v, \"Attention has incorrect shape, should be: (n_token, d_value)\"\n",
    "\n",
    "print (attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee45739-2f26-4273-8bb6-15e1933994c7",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "With scaled dot product implemented, we can go ahead and implement multihead attention.\n",
    "\n",
    "We'll focus on this part first, since there is a lot to unpack here:  \n",
    "\"_Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries,\n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned\n",
    "linear projections to d_k, d_k and d_v dimensions, respectively_\"\n",
    "\n",
    "Let's get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8714b47f-d5e4-43b9-8637-0f01d477cf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(20).view(2, 2, 5).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be974186-ce02-4e18-bc45-ee65158b2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Computes multi head attention\n",
    "        \"\"\"\n",
    "        d_k = d_v = x.shape[-1] // self.n_heads\n",
    "        query_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_k))\n",
    "        key_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_k))\n",
    "        value_projections = nn.Parameter(torch.randn(self.n_heads, d_model, d_v))\n",
    "        output_projection = nn.Parameter(torch.randn(d_v * self.n_heads, d_model))\n",
    "    \n",
    "        head_outputs = []\n",
    "        for i in range(self.n_heads):\n",
    "            q_proj = x @ query_projections[i]\n",
    "            k_proj = x @ key_projections[i]\n",
    "            v_proj = x @ value_projections[i]\n",
    "            head_output = scaled_dot_product_attention(q_proj, k_proj, v_proj)\n",
    "            head_outputs.append(head_output)\n",
    "    \n",
    "        concat_output = torch.concat(head_outputs, dim=-1)  \n",
    "        return concat_output @ output_projection    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0d45b7c-c319-49c8-84b8-b874d56e4ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0940,  0.2034, -0.5026,  ..., -1.4236, -0.7154,  0.0523],\n",
       "        [ 1.2520, -0.3363,  0.1674,  ...,  0.4714, -1.7212,  1.3816],\n",
       "        [-0.7531, -0.0715, -0.2196,  ..., -1.9276,  0.9531,  1.9429],\n",
       "        ...,\n",
       "        [ 2.0618, -0.2677, -1.2191,  ..., -1.6306,  1.1393,  0.6736],\n",
       "        [ 0.1889,  0.6635, -1.3172,  ...,  1.4840,  1.0351,  0.6650],\n",
       "        [ 0.3105,  0.5503,  0.3433,  ..., -0.4514, -1.1513, -0.6309]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_context = 64\n",
    "\n",
    "values = torch.randn(d_context, d_model)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc035999-cc6c-4621-8f62-4512d25cd45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.5218e+02, -3.1108e+02, -3.0129e+02,  ...,  4.0344e-01,\n",
       "         -4.8299e+02, -1.6218e+02],\n",
       "        [ 3.3179e+02,  1.8065e+02, -4.0647e+02,  ..., -7.4219e+02,\n",
       "          3.7257e+02,  2.6203e+02],\n",
       "        [ 2.2212e+02,  6.6790e+02, -2.0758e+02,  ..., -3.6973e+02,\n",
       "         -2.8809e+02, -3.5029e+02],\n",
       "        ...,\n",
       "        [ 5.5337e+02,  5.3414e+01,  2.4222e+02,  ...,  6.6526e+02,\n",
       "         -3.2101e+02,  1.3975e+02],\n",
       "        [ 8.2053e+02, -2.7026e+02, -7.3547e+02,  ...,  5.8289e+02,\n",
       "          1.0888e+02,  6.2960e+01],\n",
       "        [ 9.4588e+02, -6.3049e+02, -5.1539e+02,  ..., -4.1804e+02,\n",
       "          4.9018e+02, -5.8086e+02]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = MultiHeadAttention()\n",
    "attention(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199057be-9a62-4f7b-98b1-a86310337c4a",
   "metadata": {},
   "source": [
    "## Tokenization & Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b685daab-633f-45da-a294-b26191406646",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int) -> None:\n",
    "        \"\"\" \"\"\"\n",
    "        self.embedding_matrix = torch.nn.Parameter(torch.randn((vocab_size, embed_size)))\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Produces an embedding of the specified token sequence\n",
    "\n",
    "        Args:\n",
    "            tokens: a sequence of tokens of shape (max_seq_len, vocab_size)\n",
    "        \"\"\"\n",
    "        return tokens @ self.embedding_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3ec27-acb8-4f48-8acb-a1bc0155a13c",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2af2460-95c2-4963-a47a-d12fa22e8db5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.0000e+00, 1.0366e+00, 1.0366e+00, 1.0746e+00, 1.0746e+00,\n",
       "        1.1140e+00, 1.1140e+00, 1.1548e+00, 1.1548e+00, 1.1971e+00, 1.1971e+00,\n",
       "        1.2409e+00, 1.2409e+00, 1.2864e+00, 1.2864e+00, 1.3335e+00, 1.3335e+00,\n",
       "        1.3824e+00, 1.3824e+00, 1.4330e+00, 1.4330e+00, 1.4855e+00, 1.4855e+00,\n",
       "        1.5399e+00, 1.5399e+00, 1.5963e+00, 1.5963e+00, 1.6548e+00, 1.6548e+00,\n",
       "        1.7154e+00, 1.7154e+00, 1.7783e+00, 1.7783e+00, 1.8434e+00, 1.8434e+00,\n",
       "        1.9110e+00, 1.9110e+00, 1.9810e+00, 1.9810e+00, 2.0535e+00, 2.0535e+00,\n",
       "        2.1288e+00, 2.1288e+00, 2.2067e+00, 2.2067e+00, 2.2876e+00, 2.2876e+00,\n",
       "        2.3714e+00, 2.3714e+00, 2.4582e+00, 2.4582e+00, 2.5483e+00, 2.5483e+00,\n",
       "        2.6416e+00, 2.6416e+00, 2.7384e+00, 2.7384e+00, 2.8387e+00, 2.8387e+00,\n",
       "        2.9427e+00, 2.9427e+00, 3.0505e+00, 3.0505e+00, 3.1623e+00, 3.1623e+00,\n",
       "        3.2781e+00, 3.2781e+00, 3.3982e+00, 3.3982e+00, 3.5227e+00, 3.5227e+00,\n",
       "        3.6517e+00, 3.6517e+00, 3.7855e+00, 3.7855e+00, 3.9242e+00, 3.9242e+00,\n",
       "        4.0679e+00, 4.0679e+00, 4.2170e+00, 4.2170e+00, 4.3714e+00, 4.3714e+00,\n",
       "        4.5316e+00, 4.5316e+00, 4.6976e+00, 4.6976e+00, 4.8697e+00, 4.8697e+00,\n",
       "        5.0481e+00, 5.0481e+00, 5.2330e+00, 5.2330e+00, 5.4247e+00, 5.4247e+00,\n",
       "        5.6234e+00, 5.6234e+00, 5.8294e+00, 5.8294e+00, 6.0430e+00, 6.0430e+00,\n",
       "        6.2643e+00, 6.2643e+00, 6.4938e+00, 6.4938e+00, 6.7317e+00, 6.7317e+00,\n",
       "        6.9783e+00, 6.9783e+00, 7.2339e+00, 7.2339e+00, 7.4989e+00, 7.4989e+00,\n",
       "        7.7737e+00, 7.7737e+00, 8.0584e+00, 8.0584e+00, 8.3536e+00, 8.3536e+00,\n",
       "        8.6596e+00, 8.6596e+00, 8.9769e+00, 8.9769e+00, 9.3057e+00, 9.3057e+00,\n",
       "        9.6466e+00, 9.6466e+00, 1.0000e+01, 1.0000e+01, 1.0366e+01, 1.0366e+01,\n",
       "        1.0746e+01, 1.0746e+01, 1.1140e+01, 1.1140e+01, 1.1548e+01, 1.1548e+01,\n",
       "        1.1971e+01, 1.1971e+01, 1.2409e+01, 1.2409e+01, 1.2864e+01, 1.2864e+01,\n",
       "        1.3335e+01, 1.3335e+01, 1.3824e+01, 1.3824e+01, 1.4330e+01, 1.4330e+01,\n",
       "        1.4855e+01, 1.4855e+01, 1.5399e+01, 1.5399e+01, 1.5963e+01, 1.5963e+01,\n",
       "        1.6548e+01, 1.6548e+01, 1.7154e+01, 1.7154e+01, 1.7783e+01, 1.7783e+01,\n",
       "        1.8434e+01, 1.8434e+01, 1.9110e+01, 1.9110e+01, 1.9810e+01, 1.9810e+01,\n",
       "        2.0535e+01, 2.0535e+01, 2.1288e+01, 2.1288e+01, 2.2067e+01, 2.2067e+01,\n",
       "        2.2876e+01, 2.2876e+01, 2.3714e+01, 2.3714e+01, 2.4582e+01, 2.4582e+01,\n",
       "        2.5483e+01, 2.5483e+01, 2.6416e+01, 2.6416e+01, 2.7384e+01, 2.7384e+01,\n",
       "        2.8387e+01, 2.8387e+01, 2.9427e+01, 2.9427e+01, 3.0505e+01, 3.0505e+01,\n",
       "        3.1623e+01, 3.1623e+01, 3.2781e+01, 3.2781e+01, 3.3982e+01, 3.3982e+01,\n",
       "        3.5227e+01, 3.5227e+01, 3.6517e+01, 3.6517e+01, 3.7855e+01, 3.7855e+01,\n",
       "        3.9242e+01, 3.9242e+01, 4.0679e+01, 4.0679e+01, 4.2170e+01, 4.2170e+01,\n",
       "        4.3714e+01, 4.3714e+01, 4.5316e+01, 4.5316e+01, 4.6976e+01, 4.6976e+01,\n",
       "        4.8697e+01, 4.8697e+01, 5.0481e+01, 5.0481e+01, 5.2330e+01, 5.2330e+01,\n",
       "        5.4247e+01, 5.4247e+01, 5.6234e+01, 5.6234e+01, 5.8294e+01, 5.8294e+01,\n",
       "        6.0430e+01, 6.0430e+01, 6.2643e+01, 6.2643e+01, 6.4938e+01, 6.4938e+01,\n",
       "        6.7317e+01, 6.7317e+01, 6.9783e+01, 6.9783e+01, 7.2339e+01, 7.2339e+01,\n",
       "        7.4989e+01, 7.4989e+01, 7.7737e+01, 7.7737e+01, 8.0584e+01, 8.0584e+01,\n",
       "        8.3536e+01, 8.3536e+01, 8.6596e+01, 8.6596e+01, 8.9769e+01, 8.9769e+01,\n",
       "        9.3057e+01, 9.3057e+01, 9.6466e+01, 9.6466e+01, 1.0000e+02, 1.0000e+02,\n",
       "        1.0366e+02, 1.0366e+02, 1.0746e+02, 1.0746e+02, 1.1140e+02, 1.1140e+02,\n",
       "        1.1548e+02, 1.1548e+02, 1.1971e+02, 1.1971e+02, 1.2409e+02, 1.2409e+02,\n",
       "        1.2864e+02, 1.2864e+02, 1.3335e+02, 1.3335e+02, 1.3824e+02, 1.3824e+02,\n",
       "        1.4330e+02, 1.4330e+02, 1.4855e+02, 1.4855e+02, 1.5399e+02, 1.5399e+02,\n",
       "        1.5963e+02, 1.5963e+02, 1.6548e+02, 1.6548e+02, 1.7154e+02, 1.7154e+02,\n",
       "        1.7783e+02, 1.7783e+02, 1.8434e+02, 1.8434e+02, 1.9110e+02, 1.9110e+02,\n",
       "        1.9810e+02, 1.9810e+02, 2.0535e+02, 2.0535e+02, 2.1288e+02, 2.1288e+02,\n",
       "        2.2067e+02, 2.2067e+02, 2.2876e+02, 2.2876e+02, 2.3714e+02, 2.3714e+02,\n",
       "        2.4582e+02, 2.4582e+02, 2.5483e+02, 2.5483e+02, 2.6416e+02, 2.6416e+02,\n",
       "        2.7384e+02, 2.7384e+02, 2.8387e+02, 2.8387e+02, 2.9427e+02, 2.9427e+02,\n",
       "        3.0505e+02, 3.0505e+02, 3.1623e+02, 3.1623e+02, 3.2781e+02, 3.2781e+02,\n",
       "        3.3982e+02, 3.3982e+02, 3.5227e+02, 3.5227e+02, 3.6517e+02, 3.6517e+02,\n",
       "        3.7855e+02, 3.7855e+02, 3.9242e+02, 3.9242e+02, 4.0679e+02, 4.0679e+02,\n",
       "        4.2170e+02, 4.2170e+02, 4.3714e+02, 4.3714e+02, 4.5316e+02, 4.5316e+02,\n",
       "        4.6976e+02, 4.6976e+02, 4.8697e+02, 4.8697e+02, 5.0481e+02, 5.0481e+02,\n",
       "        5.2330e+02, 5.2330e+02, 5.4247e+02, 5.4247e+02, 5.6234e+02, 5.6234e+02,\n",
       "        5.8294e+02, 5.8294e+02, 6.0430e+02, 6.0430e+02, 6.2643e+02, 6.2643e+02,\n",
       "        6.4938e+02, 6.4938e+02, 6.7317e+02, 6.7317e+02, 6.9783e+02, 6.9783e+02,\n",
       "        7.2339e+02, 7.2339e+02, 7.4989e+02, 7.4989e+02, 7.7737e+02, 7.7737e+02,\n",
       "        8.0584e+02, 8.0584e+02, 8.3536e+02, 8.3536e+02, 8.6596e+02, 8.6596e+02,\n",
       "        8.9769e+02, 8.9769e+02, 9.3057e+02, 9.3057e+02, 9.6466e+02, 9.6466e+02,\n",
       "        1.0000e+03, 1.0000e+03, 1.0366e+03, 1.0366e+03, 1.0746e+03, 1.0746e+03,\n",
       "        1.1140e+03, 1.1140e+03, 1.1548e+03, 1.1548e+03, 1.1971e+03, 1.1971e+03,\n",
       "        1.2409e+03, 1.2409e+03, 1.2864e+03, 1.2864e+03, 1.3335e+03, 1.3335e+03,\n",
       "        1.3824e+03, 1.3824e+03, 1.4330e+03, 1.4330e+03, 1.4855e+03, 1.4855e+03,\n",
       "        1.5399e+03, 1.5399e+03, 1.5963e+03, 1.5963e+03, 1.6548e+03, 1.6548e+03,\n",
       "        1.7154e+03, 1.7154e+03, 1.7783e+03, 1.7783e+03, 1.8434e+03, 1.8434e+03,\n",
       "        1.9110e+03, 1.9110e+03, 1.9810e+03, 1.9810e+03, 2.0535e+03, 2.0535e+03,\n",
       "        2.1288e+03, 2.1288e+03, 2.2067e+03, 2.2067e+03, 2.2876e+03, 2.2876e+03,\n",
       "        2.3714e+03, 2.3714e+03, 2.4582e+03, 2.4582e+03, 2.5483e+03, 2.5483e+03,\n",
       "        2.6416e+03, 2.6416e+03, 2.7384e+03, 2.7384e+03, 2.8387e+03, 2.8387e+03,\n",
       "        2.9427e+03, 2.9427e+03, 3.0505e+03, 3.0505e+03, 3.1623e+03, 3.1623e+03,\n",
       "        3.2781e+03, 3.2781e+03, 3.3982e+03, 3.3982e+03, 3.5227e+03, 3.5227e+03,\n",
       "        3.6517e+03, 3.6517e+03, 3.7855e+03, 3.7855e+03, 3.9242e+03, 3.9242e+03,\n",
       "        4.0679e+03, 4.0679e+03, 4.2170e+03, 4.2170e+03, 4.3714e+03, 4.3714e+03,\n",
       "        4.5316e+03, 4.5316e+03, 4.6976e+03, 4.6976e+03, 4.8697e+03, 4.8697e+03,\n",
       "        5.0481e+03, 5.0481e+03, 5.2330e+03, 5.2330e+03, 5.4247e+03, 5.4247e+03,\n",
       "        5.6234e+03, 5.6234e+03, 5.8294e+03, 5.8294e+03, 6.0430e+03, 6.0430e+03,\n",
       "        6.2643e+03, 6.2643e+03, 6.4938e+03, 6.4938e+03, 6.7317e+03, 6.7317e+03,\n",
       "        6.9783e+03, 6.9783e+03, 7.2339e+03, 7.2339e+03, 7.4989e+03, 7.4989e+03,\n",
       "        7.7737e+03, 7.7737e+03, 8.0584e+03, 8.0584e+03, 8.3536e+03, 8.3536e+03,\n",
       "        8.6596e+03, 8.6596e+03, 8.9769e+03, 8.9769e+03, 9.3057e+03, 9.3057e+03,\n",
       "        9.6466e+03, 9.6466e+03])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias = torch.full((d_model,), 10_000, dtype=torch.float)\n",
    "indices = ((torch.arange(d_model) // 2) * 2) / d_model\n",
    "divisor = bias.pow(indices)\n",
    "divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90281fea-dd03-4327-912d-5c94e56eb89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 9.6466e-01,  ..., 1.0746e-04, 1.0366e-04,\n",
       "         1.0366e-04],\n",
       "        [2.0000e+00, 2.0000e+00, 1.9293e+00,  ..., 2.1492e-04, 2.0733e-04,\n",
       "         2.0733e-04],\n",
       "        ...,\n",
       "        [1.0210e+03, 1.0210e+03, 9.8492e+02,  ..., 1.0972e-01, 1.0584e-01,\n",
       "         1.0584e-01],\n",
       "        [1.0220e+03, 1.0220e+03, 9.8588e+02,  ..., 1.0982e-01, 1.0594e-01,\n",
       "         1.0594e-01],\n",
       "        [1.0230e+03, 1.0230e+03, 9.8685e+02,  ..., 1.0993e-01, 1.0605e-01,\n",
       "         1.0605e-01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = 1024\n",
    "pos = torch.arange(max_seq_len, dtype=torch.float).view((-1, 1))\n",
    "freqs = pos / divisor\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94775cc1-8bd3-44d4-9d89-b58501b12ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 1.7612e-02, -9.9984e-01, -9.9954e-01,  ...,  9.9399e-01,\n",
       "          1.0564e-01,  9.9440e-01],\n",
       "        [-8.3182e-01, -5.5504e-01, -5.4457e-01,  ...,  9.9398e-01,\n",
       "          1.0575e-01,  9.9439e-01],\n",
       "        [-9.1649e-01,  4.0007e-01,  3.7906e-01,  ...,  9.9396e-01,\n",
       "          1.0585e-01,  9.9438e-01]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.zeros((max_seq_len, d_model), dtype=torch.float)\n",
    "PE[:, 0::2] = torch.sin(freqs[:, 0::2])\n",
    "PE[:, 1::2] = torch.cos(freqs[:, 1::2])\n",
    "PE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc16cbe-a6b3-466c-a071-63610597e6a6",
   "metadata": {},
   "source": [
    "Now let's pull it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f64c90a-3a14-427e-ae21-4d1ac9e3cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncode(nn.Module):\n",
    "    def __init__(self, max_seq_length: int = 1024, d_model: int = 512) -> None:\n",
    "        \"\"\"\n",
    "        Generates sinusoidal positional encodings.\n",
    "    \n",
    "        Parameters:\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "            d_model (int): Dimensionality of the model embeddings.\n",
    "    \n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (max_seq_len, d_model) containing \n",
    "                          the positional encodings.\n",
    "        \"\"\"\n",
    "        # Create position indices: pos = [0, 1, ..., max_seq_len-1]\n",
    "        pos_indices = torch.arange(max_seq_len, dtype=torch.float32)\n",
    "        \n",
    "        # Create dimension indices: dim = [0, 1, ..., d_model-1]\n",
    "        dim_indices = torch.arange(d_model, dtype=torch.float32)\n",
    "        \n",
    "        # Compute the scaling exponent: 2 * floor(dim/2) / d_model\n",
    "        exponent = ((dim_indices // 2) * 2) / d_model\n",
    "        \n",
    "        # Compute the denominator term: 10000^(exponent)\n",
    "        div_term = torch.pow(10000, exponent)\n",
    "        \n",
    "        # Compute the angle rates: pos / div_term\n",
    "        angle_rates = pos_indices.unsqueeze(1) / div_term\n",
    "        \n",
    "        # Initialize the positional encoding matrix and apply sine to even \n",
    "        # indices and cosine to odd indices.\n",
    "        pos_encoding = torch.zeros_like(angle_rates)\n",
    "        pos_encoding[:, 0::2] = torch.sin(angle_rates[:, 0::2])\n",
    "        pos_encoding[:, 1::2] = torch.cos(angle_rates[:, 1::2])\n",
    "        \n",
    "        return pos_encoding\n",
    "\n",
    "    def forward(self):\n",
    "        return self.PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ecb928eb-012b-4a5e-a5ff-3cdafc779226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 1.7612e-02, -9.9984e-01, -9.9954e-01,  ...,  9.9399e-01,\n",
       "          1.0564e-01,  9.9440e-01],\n",
       "        [-8.3182e-01, -5.5504e-01, -5.4457e-01,  ...,  9.9398e-01,\n",
       "          1.0575e-01,  9.9439e-01],\n",
       "        [-9.1649e-01,  4.0007e-01,  3.7906e-01,  ...,  9.9396e-01,\n",
       "          1.0585e-01,  9.9438e-01]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PositionalEncode()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1769249d-ddf3-476c-bccd-23a2e9519b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_positional_encodings(max_seq_length, d_model):\n",
    "        dimensions = torch.arange(d_model)\n",
    "        positions = torch.arange(max_seq_len)\n",
    "\n",
    "        exponent = ((dimensions // 2) * 2) / d_model\n",
    "        freq_divisor = torch.full_like(dimensions, 10_000).pow(exponent)\n",
    "        freqs = positions.view((max_seq_len, 1)) / freq_divisor\n",
    "        position_encoding = torch.zeros_like(freqs)\n",
    "        position_encoding[:, 0::2] = torch.sin(freqs[:, 0::2])\n",
    "        position_encoding[:, 1::2] = torch.cos(freqs[:, 1::2])\n",
    "        return position_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47803e76-07b7-4a6f-9a95-f90043e1f121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "          0.0000e+00,  1.0000e+00],\n",
       "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "          1.0366e-04,  1.0000e+00],\n",
       "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "          2.0733e-04,  1.0000e+00],\n",
       "        ...,\n",
       "        [ 1.7612e-02, -9.9984e-01, -9.9954e-01,  ...,  9.9399e-01,\n",
       "          1.0564e-01,  9.9440e-01],\n",
       "        [-8.3182e-01, -5.5504e-01, -5.4457e-01,  ...,  9.9398e-01,\n",
       "          1.0575e-01,  9.9439e-01],\n",
       "        [-9.1649e-01,  4.0007e-01,  3.7906e-01,  ...,  9.9396e-01,\n",
       "          1.0585e-01,  9.9438e-01]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_positional_encodings(1024, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d1ce4-4fee-4fc5-8111-2e3d7d99e63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
