{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcfd8cb-bc8f-4785-938a-0626cd63c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import datasets\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "import tokenizers\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6144fd85-2e00-425d-9036-8af103caf44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "CONTEXT_LENGTH = 512\n",
    "VOCAB_SIZE = 16_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e17f4e4-7f83-4b44-8d94-43f84a4d9747",
   "metadata": {},
   "source": [
    "## Load The Dataset\n",
    "\n",
    "For the purposes of our simple next token prediction transformer we'll keep it to a simple and small dataset with an equally small tokenizer to keep models size small.\n",
    "\n",
    "I sought out this dataset for a couple reasons:\n",
    "1. It's english only removing the need to handle unicode (although the byte level bpe would handle this already)\n",
    "2. It's small with a limit on the number of words which helps us to limit the number of tokens and keeps the transformer params small and quicker to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f902ee08-eef7-47fe-8b18-d69749ce6e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f53addb-17d2-4936-9893-3139e7dffcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' = Valkyria Chronicles III = \\n',\n",
       " '',\n",
       " ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\",\n",
       " \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\",\n",
       " '',\n",
       " ' = = Gameplay = = \\n',\n",
       " '',\n",
       " \" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e1f2d-2804-49fb-aea0-74eb9bf51e18",
   "metadata": {},
   "source": [
    "## Configure The Tokenizer\n",
    "\n",
    "For this we'll be using a byte level byte pair encoding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2691dd7f-1499-4507-9647-7f3b5365e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, min_frequency=2)\n",
    "\n",
    "if tokenizer.get_vocab_size() < VOCAB_SIZE:\n",
    "    pass  # We should do something here to prevent this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec098d9-220d-45fb-b8bd-6cfea5c941ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size: int = BATCH_SIZE) -> list[str]:\n",
    "    for batch in dataset['train'].select_columns(\"text\").iter(batch_size):\n",
    "       yield batch['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d973e43-bd27-4211-bb74-c16019615b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f0657f-e8ba-4d2c-8eea-fc85547bd542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġ=', 'ĠV', 'alk', 'y', 'ria', 'ĠChronic', 'les', 'ĠIII', 'Ġ=', 'ĠĊ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(dataset[\"train\"][1][\"text\"]).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bcc3829-d9de-4fc6-95aa-5897611b0ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_batches(\n",
    "    dataset: datasets.Dataset, \n",
    "    tokenizer: tokenizers.Tokenizer, \n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    context_length: int = CONTEXT_LENGTH,\n",
    ") -> torch.Tensor:\n",
    "    buffer = torch.zeros(batch_size, context_length, VOCAB_SIZE, dtype=torch.int16) \n",
    "\n",
    "    write_ix = 0\n",
    "    token_write_ix = 0\n",
    "    \n",
    "    for sample in dataset:\n",
    "        token_ids = tokenizer.encode(sample[\"text\"]).ids\n",
    "        for token_id in token_ids:\n",
    "            buffer[write_ix, token_write_ix, token_id] = 1\n",
    "            token_write_ix += 1\n",
    "        \n",
    "            if token_write_ix == context_length:\n",
    "                token_write_ix = 0\n",
    "                write_ix += 1\n",
    "    \n",
    "            if write_ix == batch_size:\n",
    "                write_ix = 0\n",
    "                yield buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5734942f-7c26-4f81-97bc-2cbac8be0a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int16)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_iter = iter_batches(dataset['train'], tokenizer, batch_size=6, context_length=24)\n",
    "batch = next(batch_iter)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c5c6c5c-60be-4430-be71-8041c2971fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_batch(batch: torch.Tensor) -> list[str]:\n",
    "    return [tokenizer.decode(torch.argmax(sample, dim=-1).tolist()) for sample in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44206235-b27a-4e9a-87e1-ef005ded13a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
